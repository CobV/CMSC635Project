{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9612ee14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Wed Mar  8 12:04:48 2023\n",
    "@author: Ahmad Al Musawi\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve, average_precision_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cbe7484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_labels(df, cols):\n",
    "    '''split the dataframe into predicting table and labels\n",
    "       df: given dataset\n",
    "       cols: list of labels\n",
    "    '''\n",
    "    return df[[i for i in df if i not in cols]], df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0755103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearSVM(X_train,y_train, X_test):\n",
    "    clf = LinearSVC(random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # make predictions on the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "def GaussianSVM(X_train,y_train, X_test):\n",
    "#     print('implementing SVM...')\n",
    "    clf = SVC(kernel='rbf', C=1.0) # Gaussian radial basis function (RBF) kernel\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "def PolySVM(X_train,y_train, X_test):\n",
    "#     print('implementing SVM...')\n",
    "    clf = SVC(kernel='poly', degree=2, coef0=1, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "def SigmoidSVM(X_train,y_train, X_test):\n",
    "#     print('implementing SVM...')\n",
    "    clf = SVC(kernel='sigmoid', gamma='scale', random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "def NaiveBayes(X_train,y_train, X_test):\n",
    "#     print('implementing Naive Bayes...')\n",
    "    clf = GaussianNB()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "def Logistic(X_train,y_train, X_test):\n",
    "#     print('implementing Logistic Regression...')\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # make predictions on the test set\n",
    "    y_pred = clf.predict(X_test)    \n",
    "    return y_pred\n",
    "\n",
    "   \n",
    "def CART(X_train,y_train, X_test):\n",
    "#     print('implementing CART...')\n",
    "    clf = DecisionTreeClassifier(max_depth=5)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "def kNN(X_train,y_train, X_test):\n",
    "#     print('implementing kNN...')\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "    # assuming X is your data and k is the number of clusters\n",
    "    clf = KNeighborsClassifier(n_neighbors=10)\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2387180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_split(X, Y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test \n",
    "\n",
    "def get_classification_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    num_classes = cm.shape[0]\n",
    "    sensitivity = []\n",
    "    specificity = []\n",
    "    for i in range(num_classes):\n",
    "        tp = cm[i,i]\n",
    "        fn = np.sum(cm[i,:]) - tp\n",
    "        fp = np.sum(cm[:,i]) - tp\n",
    "        tn = np.sum(cm) - tp - fp - fn\n",
    "        \n",
    "        sensitivity_i = tp / (tp + fn)\n",
    "        specificity_i = tn / (tn + fp)\n",
    "        sensitivity.append(sensitivity_i)\n",
    "        specificity.append(specificity_i)\n",
    "    \n",
    "    macro_sensitivity = np.mean(sensitivity)\n",
    "    macro_specificity = np.mean(specificity)\n",
    "    \n",
    "    return accuracy, macro_sensitivity, macro_specificity\n",
    "\n",
    "def predict(X,Y):\n",
    "    X_train, X_test, y_train, y_test = one_split(X, Y)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    pred_Y = [pred(X_train,y_train, X_test) for pred in predictors]\n",
    "    return [get_classification_metrics(y_test, p) for p in pred_Y]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4f6947",
   "metadata": {},
   "source": [
    "# Handling Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ecead60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df):\n",
    "    # calculate the IQR for each column\n",
    "    Q1 = df.quantile(0.25)\n",
    "    Q3 = df.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    df_clean = pd.DataFrame()\n",
    "    for col in df.columns:\n",
    "        lower = Q1[col] - 1.5 * IQR[col]\n",
    "        upper = Q3[col] + 1.5 * IQR[col]\n",
    "        df_clean[col] = df[(df[col] >= lower) & (df[col] <= upper)][col]\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9bf3d9",
   "metadata": {},
   "source": [
    "# Data imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "009a1125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "def data_imputation(X):\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    imputer.fit(X)\n",
    "    X_imputed = pd.DataFrame(imputer.transform(X), columns=X.columns)\n",
    "    return X_imputed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d2f17b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = remove_outliers(df1)\n",
    "# df1 = data_imputation(df1)\n",
    "\n",
    "# df2 = remove_outliers(df2)\n",
    "# df2 = data_imputation(df2)\n",
    "\n",
    "# df1 = pd.read_excel('cleveland data.xlsx')\n",
    "df1 = pd.read_csv('heart.csv')\n",
    "\n",
    "# Preprocessing\n",
    "dataset = df1\n",
    "from pandas import get_dummies\n",
    "a = pd.get_dummies(dataset['sex'], prefix = \"sex\")\n",
    "b = pd.get_dummies(dataset['cp'], prefix = \"cp\")\n",
    "c = pd.get_dummies(dataset['fbs'], prefix = \"fbs\")\n",
    "d = pd.get_dummies(dataset['restecg'], prefix = \"restecg\")\n",
    "e = pd.get_dummies(dataset['exang'], prefix = \"exang\")\n",
    "f = pd.get_dummies(dataset['slope'], prefix = \"slope\")\n",
    "g = pd.get_dummies(dataset['ca'], prefix = \"ca\")\n",
    "h = pd.get_dummies(dataset['thal'], prefix = \"thal\")\n",
    "\n",
    "frames = [dataset, a, b, c, d, e, f, g, h]\n",
    "dataset2 = pd.concat(frames, axis = 1)\n",
    "dataset2 = dataset2.drop(columns = ['sex','cp', 'fbs', 'restecg','exang','slope','ca','thal'])\n",
    "\n",
    "df1 = dataset2\n",
    "X1, Y1 = split_labels(df1, ['target'])\n",
    "\n",
    "# nX1 = stats.zscore(X1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31ececaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text file into a DataFrame\n",
    "# df1 = pd.read_csv('processed.cleveland.data', delimiter=',', header=None)\n",
    "# df1 = pd.read_excel('cleveland data.xlsx')\n",
    "\n",
    "df2 = pd.read_excel('CTG.xls', sheet_name = 'Raw Data')\n",
    "\n",
    "df2 = df2[[i for i in df2 if i not in ['FileName','Date','SegFile']]]\n",
    "\n",
    "X2, Y2 = split_labels(df2, ['NSP'])\n",
    "\n",
    "nX2 = stats.zscore(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4b52907",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heart Disease\n",
      "         model  accuracy  sensitivity  specificity\n",
      "0    LinearSVM  0.814935     0.816323     0.816323\n",
      "1  GaussianSVM  0.918831     0.919484     0.919484\n",
      "2      PolySVM  0.870130     0.870837     0.870837\n",
      "3   SigmoidSVM  0.795455     0.797877     0.797877\n",
      "4   NaiveBayes  0.733766     0.740239     0.740239\n",
      "5     Logistic  0.798701     0.799755     0.799755\n",
      "6         CART  0.879870     0.882593     0.882593\n",
      "7          kNN  0.863636     0.863704     0.863704\n",
      "Heart Disease2\n",
      "         model  accuracy  sensitivity  specificity\n",
      "0    LinearSVM  0.989028     0.979953     0.990475\n",
      "1  GaussianSVM  0.985893     0.973352     0.985780\n",
      "2      PolySVM  0.989028     0.979953     0.990475\n",
      "3   SigmoidSVM  0.957680     0.933625     0.968573\n",
      "4   NaiveBayes  0.913793     0.947695     0.960680\n",
      "5     Logistic  0.989028     0.979953     0.990475\n",
      "6         CART  0.971787     0.964675     0.978467\n",
      "7          kNN  0.985893     0.973352     0.985780\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# predictors = [ NaiveBayes, Logistic, CART, kNN]\n",
    "# predictorsTXT = ['NaiveBayes', 'Logistic', 'CART', 'kNN']\n",
    "predictors = [LinearSVM, GaussianSVM, PolySVM, SigmoidSVM, NaiveBayes, Logistic, CART, kNN]\n",
    "predictorsTXT = ['LinearSVM', 'GaussianSVM', 'PolySVM', 'SigmoidSVM', 'NaiveBayes', 'Logistic', 'CART', 'kNN']\n",
    "\n",
    "results1 = predict(X1, Y1)\n",
    "results2 = predict(X2, Y2)\n",
    "\n",
    "print('Heart Disease')\n",
    "acc, sen, spe  = [],[],[]\n",
    "for a, s, e in results1:\n",
    "    acc.append(a)\n",
    "    sen.append(s)\n",
    "    spe.append(e)\n",
    "print(pd.DataFrame({'model': predictorsTXT,'accuracy': acc, 'sensitivity': sen, 'specificity': spe}))\n",
    "\n",
    "\n",
    "print('Heart Disease2')\n",
    "acc, sen, spe = [],[],[]\n",
    "for a, s, e  in results2:\n",
    "    acc.append(a)\n",
    "    sen.append(s)\n",
    "    spe.append(e)\n",
    "print(pd.DataFrame({'model': predictorsTXT,'accuracy': acc, 'sensitivity': sen, 'specificity': spe}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10e1e673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_model(X, y=None, n = 2):\n",
    "#     print(\"PCA model\")\n",
    "    pca = PCA(n_components=n)\n",
    "    pca.fit(X)\n",
    "    X_pca = pca.transform(X)\n",
    "    print(pca.explained_variance_ratio_)\n",
    "    selected_features = pca.components_\n",
    "    print(f'PCA\\tNoF = {len(selected_features)}')\n",
    "    return X_pca\n",
    "\n",
    "def Kernel_PCA(X, y=None, n = 2):\n",
    "    from sklearn.decomposition import KernelPCA\n",
    "#     print(\"Kernal PCA model\")\n",
    "    pca = KernelPCA(n_components=n, kernel='rbf')\n",
    "    pca.fit_transform(X)\n",
    "    return pca\n",
    "\n",
    "def CE_Model(X, y=None, n=2):\n",
    "#     print('CE Model')\n",
    "    embedding = SpectralEmbedding(n_components=n)\n",
    "    X_CE = embedding.fit_transform(X)\n",
    "    print(f'CE\\tOld shape = {X.shape}\\t\\t new shape = {X_CE.shape}\\t\\t components = {n}')\n",
    "    return X_CE\n",
    "\n",
    "def CE2(X, y=None, n=2):\n",
    "#     print('CE Model: Laplacian Eigenmaps')\n",
    "    embedding = SpectralEmbedding(n_components=n, affinity='nearest_neighbors', n_neighbors=10, eigen_solver='arpack')\n",
    "    X_CE = embedding.fit_transform(X)\n",
    "    print(f'CE2\\tOld shape = {X.shape}\\t\\t new shape = {X_CE.shape}\\t\\t components = {n}')\n",
    "    return X_CE\n",
    "\n",
    "def LLE(X, y=None, n=2):\n",
    "    from sklearn.manifold import LocallyLinearEmbedding\n",
    "#     print('CE Model: Locally Linear Embedding')\n",
    "    embedding = LocallyLinearEmbedding(n_components=n, n_neighbors=10)\n",
    "    X_CE = embedding.fit_transform(X)\n",
    "    print(f'LLE\\tOld shape = {X.shape}\\t\\t new shape = {X_CE.shape}\\t\\t components = {n}')\n",
    "    return X_CE\n",
    "\n",
    "def Isomap(X, y=None, n=2):\n",
    "    from sklearn.manifold import Isomap\n",
    "#     print('CE Model: Isomap')\n",
    "    embedding =  Isomap(n_components=n, n_neighbors=10)\n",
    "    X_CE = embedding.fit_transform(X)\n",
    "    print(f'ISOMAP\\tOld shape = {X.shape}\\t\\t new shape = {X_CE.shape}\\t\\t components = {n}')\n",
    "    return X_CE\n",
    "\n",
    "def TSNE(X, y=None, n=2):\n",
    "    from sklearn.manifold import TSNE\n",
    "#     print('CE Model: TSNE')\n",
    "    embedding = TSNE(n_components=2, perplexity=30, n_iter=1000)\n",
    "    X_CE = embedding.fit_transform(X)\n",
    "    print(f'TSNE\\tOld shape = {X.shape}\\t\\t new shape = {X_CE.shape}\\t\\t components = {n}')\n",
    "    return X_CE\n",
    "\n",
    "\n",
    "\n",
    "def CFS(X, y, n=2):\n",
    "#     print('CFS Model')\n",
    "    selector = SelectKBest(score_func=f_regression, k=5)\n",
    "    X_new = selector.fit_transform(X, y)\n",
    "    return X_new\n",
    "\n",
    "def LLCFS(X, y=None,n=2):\n",
    "#     print('LLCFS Model')\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    return X\n",
    "\n",
    "def ILFS(X, y):\n",
    "    # create a linear regression model\n",
    "#     print('ILFS Model')\n",
    "    model = LinearRegression()\n",
    "    \n",
    "    # define the search space\n",
    "    k_features = np.arange(1, X.shape[1]+1)\n",
    "    \n",
    "    # create a sequential feature selector object\n",
    "    selector = SequentialFeatureSelector(model, k_features=k_features, forward=True, scoring='r2', cv=5)\n",
    "    \n",
    "    # perform incremental feature selection\n",
    "    selector.fit(X, y)\n",
    "    \n",
    "    # print the selected feature indices\n",
    "    print(\"Indices of selected features:\", selector.k_feature_idx_)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9250ece6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.45250270e-01 1.51989441e-01 8.49212797e-02 1.64983040e-02\n",
      " 3.79041061e-04 1.68144814e-04 1.31375727e-04 1.07866598e-04\n",
      " 9.27031645e-05 8.02546887e-05 7.18502447e-05 6.69134612e-05\n",
      " 6.06049535e-05 4.64435282e-05 3.62096578e-05 2.45265630e-05\n",
      " 2.29950682e-05 1.94622818e-05 1.90456946e-05 5.57160064e-06\n",
      " 5.27786510e-06 2.41843846e-06 4.29083350e-33 4.29083350e-33\n",
      " 4.29083350e-33 4.29083350e-33 4.29083350e-33]\n",
      "PCA\tNoF = 27\n",
      "CE\tOld shape = (1025, 30)\t\t new shape = (1025, 27)\t\t components = 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_spectral_embedding.py:260: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE2\tOld shape = (1025, 30)\t\t new shape = (1025, 27)\t\t components = 27\n",
      "LLE\tOld shape = (1025, 30)\t\t new shape = (1025, 27)\t\t components = 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_isomap.py:324: UserWarning: The number of connected components of the neighbors graph is 2 > 1. Completing the graph to fit Isomap might be slow. Increase the number of neighbors to avoid this issue.\n",
      "  self._fit_transform(X)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_index.py:103: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISOMAP\tOld shape = (1025, 30)\t\t new shape = (1025, 27)\t\t components = 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:780: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSNE\tOld shape = (1025, 30)\t\t new shape = (1025, 2)\t\t components = 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFS\n",
      "LLCFS\n",
      "PCA_model\n",
      "CE_Model\n",
      "CE2\n",
      "LLE\n",
      "Isomap\n",
      "TSNE\n",
      "LinearSVM\n",
      "          DR  accuracy  sensitivity  specificity\n",
      "0        CFS  0.818182     0.819889     0.819889\n",
      "0      LLCFS  0.814935     0.816323     0.816323\n",
      "0  PCA_model  0.824675     0.826179     0.826179\n",
      "0   CE_Model  0.665584     0.664704     0.664704\n",
      "0        CE2  0.678571     0.679604     0.679604\n",
      "0        LLE  0.707792     0.706851     0.706851\n",
      "0     Isomap  0.694805     0.696172     0.696172\n",
      "0       TSNE  0.665584     0.665126     0.665126\n",
      "\n",
      "GaussianSVM\n",
      "          DR  accuracy  sensitivity  specificity\n",
      "1        CFS  0.824675     0.826390     0.826390\n",
      "1      LLCFS  0.918831     0.919484     0.919484\n",
      "1  PCA_model  0.899351     0.899561     0.899561\n",
      "1   CE_Model  0.801948     0.802267     0.802267\n",
      "1        CE2  0.724026     0.723629     0.723629\n",
      "1        LLE  0.717532     0.716496     0.716496\n",
      "1     Isomap  0.720779     0.722806     0.722806\n",
      "1       TSNE  0.688312     0.687350     0.687350\n",
      "\n",
      "PolySVM\n",
      "          DR  accuracy  sensitivity  specificity\n",
      "2        CFS  0.821429     0.823878     0.823878\n",
      "2      LLCFS  0.870130     0.870837     0.870837\n",
      "2  PCA_model  0.870130     0.871048     0.871048\n",
      "2   CE_Model  0.750000     0.750053     0.750053\n",
      "2        CE2  0.720779     0.721751     0.721751\n",
      "2        LLE  0.720779     0.720485     0.720485\n",
      "2     Isomap  0.743506     0.745241     0.745241\n",
      "2       TSNE  0.646104     0.641615     0.641615\n",
      "\n",
      "SigmoidSVM\n",
      "          DR  accuracy  sensitivity  specificity\n",
      "3        CFS  0.714286     0.715462     0.715462\n",
      "3      LLCFS  0.795455     0.797877     0.797877\n",
      "3  PCA_model  0.756494     0.758875     0.758875\n",
      "3   CE_Model  0.558442     0.557976     0.557976\n",
      "3        CE2  0.629870     0.629691     0.629691\n",
      "3        LLE  0.590909     0.590688     0.590688\n",
      "3     Isomap  0.525974     0.526107     0.526107\n",
      "3       TSNE  0.512987     0.512895     0.512895\n",
      "\n",
      "NaiveBayes\n",
      "          DR  accuracy  sensitivity  specificity\n",
      "4        CFS  0.772727     0.773121     0.773121\n",
      "4      LLCFS  0.733766     0.740239     0.740239\n",
      "4  PCA_model  0.724026     0.727850     0.727850\n",
      "4   CE_Model  0.623377     0.622346     0.622346\n",
      "4        CE2  0.665584     0.667870     0.667870\n",
      "4        LLE  0.675325     0.676882     0.676882\n",
      "4     Isomap  0.613636     0.617555     0.617555\n",
      "4       TSNE  0.577922     0.579798     0.579798\n",
      "\n",
      "Logistic\n",
      "          DR  accuracy  sensitivity  specificity\n",
      "5        CFS  0.808442     0.809822     0.809822\n",
      "5      LLCFS  0.798701     0.799755     0.799755\n",
      "5  PCA_model  0.805195     0.806044     0.806044\n",
      "5   CE_Model  0.675325     0.674349     0.674349\n",
      "5        CE2  0.678571     0.678971     0.678971\n",
      "5        LLE  0.704545     0.703706     0.703706\n",
      "5     Isomap  0.694805     0.695116     0.695116\n",
      "5       TSNE  0.662338     0.661770     0.661770\n",
      "\n",
      "CART\n",
      "          DR  accuracy  sensitivity  specificity\n",
      "6        CFS  0.840909     0.842324     0.842324\n",
      "6      LLCFS  0.879870     0.882593     0.882593\n",
      "6  PCA_model  0.922078     0.923051     0.923051\n",
      "6   CE_Model  0.772727     0.775864     0.775864\n",
      "6        CE2  0.740260     0.739141     0.739141\n",
      "6        LLE  0.753247     0.752142     0.752142\n",
      "6     Isomap  0.782468     0.781499     0.781499\n",
      "6       TSNE  0.688312     0.687350     0.687350\n",
      "\n",
      "kNN\n",
      "          DR  accuracy  sensitivity  specificity\n",
      "7        CFS  0.814935     0.816112     0.816112\n",
      "7      LLCFS  0.863636     0.863704     0.863704\n",
      "7  PCA_model  0.824675     0.824912     0.824912\n",
      "7   CE_Model  0.704545     0.702651     0.702651\n",
      "7        CE2  0.675325     0.674349     0.674349\n",
      "7        LLE  0.694805     0.693639     0.693639\n",
      "7     Isomap  0.698052     0.698261     0.698261\n",
      "7       TSNE  0.704545     0.704128     0.704128\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "# performing experiment 2    \n",
    "\n",
    "DR = [ CFS, LLCFS, PCA_model, CE_Model, CE2, LLE, Isomap, TSNE,]\n",
    "DR_TXT = [ 'CFS', 'LLCFS', 'PCA_model', 'CE_Model', 'CE2', 'LLE', 'Isomap', 'TSNE', ]\n",
    "\n",
    "\n",
    "NoF = X1.shape[1]-3 # Number of features\n",
    "X1s = [d(X1, np.ravel(Y1), NoF) for d in DR] # dimension reduction\n",
    "results1 = [predict(x1, np.ravel(Y1)) for x1 in X1s] # Machine learning models\n",
    "\n",
    "finals = []\n",
    "for i in range(len(DR)):\n",
    "    rd = DR_TXT[i]\n",
    "    print(rd)\n",
    "    acc, sen, spe = [],[],[]\n",
    "    for a, s, e in results1[i]:\n",
    "        acc.append(a)\n",
    "        sen.append(s)\n",
    "        spe.append(e)\n",
    "    finals.append(pd.DataFrame({'model': predictorsTXT,'accuracy': acc, 'sensitivity': sen, 'specificity': spe}))\n",
    "\n",
    "\n",
    "finalR = {}\n",
    "for i in predictorsTXT:\n",
    "    D = pd.DataFrame() # create an empty DataFrame to hold the filtered rows     \n",
    "    for df in finals:\n",
    "        row = df[df['model'] == i]\n",
    "        D = D.append(row) # filter rows that match a certain condition and append them to D\n",
    "    D['DR'] = DR_TXT\n",
    "    D = D.drop('model', axis=1)\n",
    "    finalR[i] = D\n",
    "\n",
    "for i in finalR:\n",
    "    print(i)\n",
    "    print(finalR[i][['DR', 'accuracy' , 'sensitivity',  'specificity']])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e23cabc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:289: RuntimeWarning: invalid value encountered in true_divide\n",
      "  correlation_coefficient /= X_norms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.68713086e-01 2.74234961e-02 1.81937840e-03 7.64856806e-04\n",
      " 5.10486842e-04 2.95086213e-04 1.99060233e-04 1.07638969e-04\n",
      " 8.67742844e-05 3.23837027e-05 1.76585343e-05 1.28489755e-05\n",
      " 4.04401212e-06 3.54801270e-06 2.88068144e-06 2.62541850e-06\n",
      " 2.14270955e-06 1.17896369e-06 2.54867404e-07 1.58914087e-07\n",
      " 1.03668102e-07 7.68200538e-08 6.24574875e-08 5.89534241e-08\n",
      " 3.44613336e-08 2.36407207e-08 1.76699439e-08 1.57927953e-08\n",
      " 1.07041155e-08 5.87836588e-09 1.66851486e-09 4.77179627e-33\n",
      " 4.77179627e-33]\n",
      "PCA\tNoF = 33\n",
      "CE\tOld shape = (2126, 36)\t\t new shape = (2126, 33)\t\t components = 33\n",
      "CE2\tOld shape = (2126, 36)\t\t new shape = (2126, 33)\t\t components = 33\n",
      "LLE\tOld shape = (2126, 36)\t\t new shape = (2126, 33)\t\t components = 33\n",
      "ISOMAP\tOld shape = (2126, 36)\t\t new shape = (2126, 33)\t\t components = 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:780: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSNE\tOld shape = (2126, 36)\t\t new shape = (2126, 2)\t\t components = 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVM\n",
      "          DR  accuracy  sensitivity  specificity\n",
      "0        CFS  0.942006     0.896713     0.930412\n",
      "0      LLCFS  0.989028     0.979953     0.990475\n",
      "0  PCA_model  0.987461     0.976652     0.988127\n",
      "0   CE_Model  0.780564     0.345191     0.674815\n",
      "0        CE2  0.769592     0.350999     0.684283\n",
      "0        LLE  0.789969     0.399228     0.715361\n",
      "0     Isomap  0.789969     0.370249     0.697533\n",
      "0       TSNE  0.777429     0.333333     0.666667\n",
      "\n",
      "GaussianSVM\n",
      "          DR  accuracy  sensitivity  specificity\n",
      "1        CFS  0.952978     0.896160     0.931304\n",
      "1      LLCFS  0.985893     0.973352     0.985780\n",
      "1  PCA_model  0.987461     0.974024     0.986401\n",
      "1   CE_Model  0.789969     0.394398     0.697720\n",
      "1        CE2  0.783699     0.357048     0.688143\n",
      "1        LLE  0.808777     0.470865     0.739640\n",
      "1     Isomap  0.794671     0.427100     0.715122\n",
      "1       TSNE  0.777429     0.333333     0.666667\n",
      "\n",
      "PolySVM\n",
      "          DR  accuracy  sensitivity  specificity\n",
      "2        CFS  0.952978     0.896160     0.931304\n",
      "2      LLCFS  0.989028     0.979953     0.990475\n",
      "2  PCA_model  0.989028     0.979953     0.990475\n",
      "2   CE_Model  0.778997     0.395377     0.705774\n",
      "2        CE2  0.778997     0.373430     0.700094\n",
      "2        LLE  0.815047     0.486695     0.750756\n",
      "2     Isomap  0.791536     0.445928     0.726155\n",
      "2       TSNE  0.777429     0.333333     0.666667\n",
      "\n",
      "SigmoidSVM\n",
      "          DR  accuracy  sensitivity  specificity\n",
      "3        CFS  0.847962     0.748136     0.837915\n",
      "3      LLCFS  0.957680     0.933625     0.968573\n",
      "3  PCA_model  0.971787     0.924757     0.968107\n",
      "3   CE_Model  0.728840     0.367267     0.698621\n",
      "3        CE2  0.758621     0.348923     0.676797\n",
      "3        LLE  0.755486     0.395383     0.712315\n",
      "3     Isomap  0.755486     0.329181     0.668836\n",
      "3       TSNE  0.706897     0.337259     0.678031\n",
      "\n",
      "NaiveBayes\n",
      "          DR  accuracy  sensitivity  specificity\n",
      "4        CFS  0.840125     0.658537     0.760563\n",
      "4      LLCFS  0.913793     0.947695     0.960680\n",
      "4  PCA_model  0.935737     0.874638     0.967289\n",
      "4   CE_Model  0.626959     0.472184     0.737846\n",
      "4        CE2  0.565831     0.421331     0.734212\n",
      "4        LLE  0.728840     0.479497     0.765670\n",
      "4     Isomap  0.755486     0.529066     0.775411\n",
      "4       TSNE  0.777429     0.333333     0.666667\n",
      "\n",
      "Logistic\n",
      "          DR  accuracy  sensitivity  specificity\n",
      "5        CFS  0.940439     0.893412     0.928065\n",
      "5      LLCFS  0.989028     0.979953     0.990475\n",
      "5  PCA_model  0.987461     0.974024     0.986401\n",
      "5   CE_Model  0.785266     0.355092     0.692217\n",
      "5        CE2  0.769592     0.360659     0.686197\n",
      "5        LLE  0.797806     0.473192     0.748442\n",
      "5     Isomap  0.785266     0.383149     0.700913\n",
      "5       TSNE  0.777429     0.333333     0.666667\n",
      "\n",
      "CART\n",
      "          DR  accuracy  sensitivity  specificity\n",
      "6        CFS  0.984326     0.967424     0.981706\n",
      "6      LLCFS  0.971787     0.964675     0.978467\n",
      "6  PCA_model  0.962382     0.922073     0.968397\n",
      "6   CE_Model  0.783699     0.442995     0.719473\n",
      "6        CE2  0.777429     0.409621     0.703489\n",
      "6        LLE  0.816614     0.500935     0.772596\n",
      "6     Isomap  0.774295     0.410905     0.709029\n",
      "6       TSNE  0.783699     0.404851     0.702081\n",
      "\n",
      "kNN\n",
      "          DR  accuracy  sensitivity  specificity\n",
      "7        CFS  0.979624     0.957523     0.974664\n",
      "7      LLCFS  0.985893     0.973352     0.985780\n",
      "7  PCA_model  0.984326     0.962594     0.981706\n",
      "7   CE_Model  0.796238     0.390908     0.712103\n",
      "7        CE2  0.799373     0.394881     0.713406\n",
      "7        LLE  0.821317     0.483632     0.763537\n",
      "7     Isomap  0.786834     0.410531     0.731137\n",
      "7       TSNE  0.805643     0.439195     0.747282\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "nX2 = stats.zscore(X2)\n",
    "\n",
    "NoF = X2.shape[1]-3 # Number of features\n",
    "X2s = [d(X2, np.ravel(Y2), NoF) for d in DR]\n",
    "\n",
    "results2 = [predict(x2, np.ravel(Y2)) for x2 in X2s]\n",
    "finals = []\n",
    "for i in range(len(DR)):\n",
    "    rd = DR_TXT[i]\n",
    "    acc, sen, spe = [],[],[]\n",
    "    for a, s, e in results2[i]:\n",
    "        acc.append(a)\n",
    "        sen.append(s)\n",
    "        spe.append(e)\n",
    "    finals.append(pd.DataFrame({'model': predictorsTXT,'accuracy': acc, 'sensitivity': sen, 'specificity': spe}))\n",
    "    \n",
    "    \n",
    "finalR = {}\n",
    "for i in predictorsTXT:\n",
    "    D = pd.DataFrame() # create an empty DataFrame to hold the filtered rows     \n",
    "    for df in finals:\n",
    "        row = df[df['model'] == i]\n",
    "        D = D.append(row) # filter rows that match a certain condition and append them to D\n",
    "    D['DR'] = DR_TXT\n",
    "    D = D.drop('model', axis=1)\n",
    "    finalR[i] = D\n",
    "\n",
    "for i in finalR:\n",
    "    print(i)\n",
    "    print(finalR[i][['DR', 'accuracy' , 'sensitivity',  'specificity']])\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
