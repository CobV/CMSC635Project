{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9612ee14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Wed Mar  8 12:04:48 2023\n",
    "@author: Ahmad Al Musawi\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve, average_precision_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cbe7484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    print('preprocessing...')\n",
    "    return df\n",
    "\n",
    "def split_labels(df, cols):\n",
    "    '''split the dataframe into predicting table and labels\n",
    "       df: given dataset\n",
    "       cols: list of labels\n",
    "    '''\n",
    "    return df[[i for i in df if i not in cols]], df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0755103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearSVM(X_train,y_train, X_test):\n",
    "    svm = LinearSVC(random_state=42)\n",
    "    svm.fit(X_train, y_train)\n",
    "    \n",
    "    # make predictions on the test set\n",
    "    y_pred = svm.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "def GaussianSVM(X_train,y_train, X_test):\n",
    "#     print('implementing SVM...')\n",
    "    clf = SVC(kernel='rbf', C=1.0) # Gaussian radial basis function (RBF) kernel\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "def PolySVM(X_train,y_train, X_test):\n",
    "#     print('implementing SVM...')\n",
    "    clf = SVC(kernel='poly', degree=2, coef0=1, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "def SigmoidSVM(X_train,y_train, X_test):\n",
    "#     print('implementing SVM...')\n",
    "    clf = SVC(kernel='sigmoid', gamma='scale', random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "def NaiveBayes(X_train,y_train, X_test):\n",
    "#     print('implementing Naive Bayes...')\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(X_train, y_train)\n",
    "    y_pred = gnb.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "def Logistic(X_train,y_train, X_test):\n",
    "#     print('implementing Logistic Regression...')\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    \n",
    "    # make predictions on the test set\n",
    "    y_pred = lr.predict(X_test)    \n",
    "    return y_pred\n",
    "\n",
    "def CART(X_train,y_train, X_test):\n",
    "#     print('implementing CART...')\n",
    "    clf = DecisionTreeClassifier(max_depth=5)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "def kNN(X_train,y_train, X_test):\n",
    "#     print('implementing kNN...')\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "    # assuming X is your data and k is the number of clusters\n",
    "    knn = KNeighborsClassifier(n_neighbors=10)\n",
    "    knn.fit(X_train, y_train)\n",
    "    return knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31ececaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
      "0   63    1   1       145   233    1        2      150      0      2.3      3   \n",
      "1   67    1   4       160   286    0        2      108      1      1.5      2   \n",
      "2   67    1   4       120   229    0        2      129      1      2.6      2   \n",
      "3   37    1   3       130   250    0        0      187      0      3.5      3   \n",
      "4   41    0   2       130   204    0        2      172      0      1.4      1   \n",
      "\n",
      "   ca  thal  num  \n",
      "0   0     6    0  \n",
      "1   3     3    2  \n",
      "2   2     7    1  \n",
      "3   0     3    0  \n",
      "4   0     3    0  \n",
      "\n",
      "     b     e  LBE   LB  AC  FM  UC  ASTV  MSTV  ALTV  ...  C  D  E  AD  DE  \\\n",
      "0  240   357  120  120   0   0   0    73   0.5    43  ...  0  0  0   0   0   \n",
      "1    5   632  132  132   4   0   4    17   2.1     0  ...  0  0  0   1   0   \n",
      "2  177   779  133  133   2   0   5    16   2.1     0  ...  0  0  0   1   0   \n",
      "3  411  1192  134  134   2   0   6    16   2.4     0  ...  0  0  0   1   0   \n",
      "4  533  1147  132  132   4   0   5    16   2.4     0  ...  0  0  0   0   0   \n",
      "\n",
      "   LD  FS  SUSP  CLASS  NSP  \n",
      "0   0   1     0      9    2  \n",
      "1   0   0     0      6    1  \n",
      "2   0   0     0      6    1  \n",
      "3   0   0     0      6    1  \n",
      "4   0   0     0      2    1  \n",
      "\n",
      "[5 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the text file into a DataFrame\n",
    "# df1 = pd.read_csv('processed.cleveland.data', delimiter=',', header=None)\n",
    "df1 = pd.read_excel('cleveland data.xlsx')\n",
    "df2 = pd.read_excel('CTG.xls', sheet_name = 'Raw Data')\n",
    "\n",
    "df2 = df2[[i for i in df2 if i not in ['FileName','Date','SegFile']]]\n",
    "print(df1.head())\n",
    "print()\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2387180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_split(X, Y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test \n",
    "\n",
    "def get_classification_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    num_classes = cm.shape[0]\n",
    "    sensitivity = []\n",
    "    specificity = []\n",
    "    for i in range(num_classes):\n",
    "        tp = cm[i,i]\n",
    "        fn = np.sum(cm[i,:]) - tp\n",
    "        fp = np.sum(cm[:,i]) - tp\n",
    "        tn = np.sum(cm) - tp - fp - fn\n",
    "        \n",
    "        sensitivity_i = tp / (tp + fn)\n",
    "        specificity_i = tn / (tn + fp)\n",
    "        sensitivity.append(sensitivity_i)\n",
    "        specificity.append(specificity_i)\n",
    "    \n",
    "    macro_sensitivity = np.mean(sensitivity)\n",
    "    macro_specificity = np.mean(specificity)\n",
    "    \n",
    "    return accuracy, macro_sensitivity, macro_specificity\n",
    "\n",
    "def predict(X,Y):\n",
    "    X_train, X_test, y_train, y_test = one_split(X, Y)\n",
    "    pred_Y = [pred(X_train,y_train, X_test) for pred in predictors]\n",
    "    return [get_classification_metrics(y_test, p) for p in pred_Y]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c01b478",
   "metadata": {},
   "source": [
    "# Handling Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5250c304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df):\n",
    "    # calculate the IQR for each column\n",
    "    Q1 = df.quantile(0.25)\n",
    "    Q3 = df.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    df_clean = pd.DataFrame()\n",
    "    for col in df.columns:\n",
    "        lower = Q1[col] - 1.5 * IQR[col]\n",
    "        upper = Q3[col] + 1.5 * IQR[col]\n",
    "        df_clean[col] = df[(df[col] >= lower) & (df[col] <= upper)][col]\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f75735",
   "metadata": {},
   "source": [
    "# Data imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c81ad27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "def data_imputation(X):\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    imputer.fit(X)\n",
    "    X_imputed = pd.DataFrame(imputer.transform(X), columns=X.columns)\n",
    "    return X_imputed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4b52907",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heart Disease\n",
      "         model  accuracy  sensitivity  specificity\n",
      "0    LinearSVM  0.538462     0.216667     0.813254\n",
      "1  GaussianSVM  0.527473     0.200000     0.800000\n",
      "2      PolySVM  0.527473     0.200000     0.800000\n",
      "3   SigmoidSVM  0.527473     0.200000     0.800000\n",
      "4   NaiveBayes  0.516484     0.311961     0.872932\n",
      "5     Logistic  0.582418     0.318824     0.872913\n",
      "6         CART  0.516484     0.254559     0.862713\n",
      "7          kNN  0.527473     0.231029     0.830503\n",
      "Heart Disease2\n",
      "         model  accuracy  sensitivity  specificity\n",
      "0    LinearSVM  0.528213     0.436226     0.943564\n",
      "1  GaussianSVM  0.266458     0.100847     0.900213\n",
      "2      PolySVM  0.271160     0.104301     0.901031\n",
      "3   SigmoidSVM  0.233542     0.120081     0.904146\n",
      "4   NaiveBayes  0.746082     0.759912     0.972915\n",
      "5     Logistic  0.489028     0.311814     0.936286\n",
      "6         CART  0.865204     0.694194     0.984264\n",
      "7          kNN  0.341693     0.218845     0.916834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "X1, Y1 = split_labels(df1, ['num'])\n",
    "X2, Y2 = split_labels(df2, ['CLASS'])\n",
    "\n",
    "X1 = remove_outliers(X1)\n",
    "X1 = data_imputation(X1)\n",
    "nX1 = stats.zscore(X1)\n",
    "\n",
    "X2 = remove_outliers(X2)\n",
    "X2 = data_imputation(X2)\n",
    "nX2 = stats.zscore(X2)\n",
    "\n",
    "# predictors = [ NaiveBayes, Logistic, CART, kNN]\n",
    "# predictorsTXT = ['NaiveBayes', 'Logistic', 'CART', 'kNN']\n",
    "predictors = [LinearSVM, GaussianSVM, PolySVM, SigmoidSVM, NaiveBayes, Logistic, CART, kNN]\n",
    "predictorsTXT = ['LinearSVM', 'GaussianSVM', 'PolySVM', 'SigmoidSVM', 'NaiveBayes', 'Logistic', 'CART', 'kNN']\n",
    "\n",
    "results1 = predict(X1, Y1)\n",
    "results2 = predict(X2, Y2)\n",
    "\n",
    "print('Heart Disease')\n",
    "acc, sen, spe = [],[],[]\n",
    "for a, s, e in results1:\n",
    "    acc.append(a)\n",
    "    sen.append(s)\n",
    "    spe.append(e)\n",
    "print(pd.DataFrame({'model': predictorsTXT,'accuracy': acc, 'sensitivity': sen, 'specificity': spe}))\n",
    "\n",
    "\n",
    "print('Heart Disease2')\n",
    "acc, sen, spe = [],[],[]\n",
    "for a, s, e  in results2:\n",
    "    acc.append(a)\n",
    "    sen.append(s)\n",
    "    spe.append(e)\n",
    "print(pd.DataFrame({'model': predictorsTXT,'accuracy': acc, 'sensitivity': sen, 'specificity': spe}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10e1e673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_model(X, y=None, n = 2):\n",
    "#     print(\"PCA model\")\n",
    "    pca = PCA(n_components=n)\n",
    "    pca.fit(X)\n",
    "    X_pca = pca.transform(X)\n",
    "    print(pca.explained_variance_ratio_)\n",
    "    selected_features = pca.components_\n",
    "    print(f'PCA\\tNoF = {len(selected_features)}')\n",
    "    return X_pca\n",
    "\n",
    "def Kernel_PCA(X, y=None, n = 2):\n",
    "    from sklearn.decomposition import KernelPCA\n",
    "#     print(\"Kernal PCA model\")\n",
    "    pca = KernelPCA(n_components=n, kernel='rbf')\n",
    "    pca.fit_transform(X)\n",
    "    return pca\n",
    "\n",
    "def CE_Model(X, y=None, n=2):\n",
    "#     print('CE Model')\n",
    "    embedding = SpectralEmbedding(n_components=n)\n",
    "    X_CE = embedding.fit_transform(X)\n",
    "    print(f'CE\\tOld shape = {X.shape}\\t\\t new shape = {X_CE.shape}\\t\\t components = {n}')\n",
    "    return X_CE\n",
    "\n",
    "def CE2(X, y=None, n=2):\n",
    "#     print('CE Model: Laplacian Eigenmaps')\n",
    "    embedding = SpectralEmbedding(n_components=n, affinity='nearest_neighbors', n_neighbors=10, eigen_solver='arpack')\n",
    "    X_CE = embedding.fit_transform(X)\n",
    "    print(f'CE2\\tOld shape = {X.shape}\\t\\t new shape = {X_CE.shape}\\t\\t components = {n}')\n",
    "    return X_CE\n",
    "\n",
    "def LLE(X, y=None, n=2):\n",
    "    from sklearn.manifold import LocallyLinearEmbedding\n",
    "#     print('CE Model: Locally Linear Embedding')\n",
    "    embedding = LocallyLinearEmbedding(n_components=n, n_neighbors=10)\n",
    "    X_CE = embedding.fit_transform(X)\n",
    "    print(f'LLE\\tOld shape = {X.shape}\\t\\t new shape = {X_CE.shape}\\t\\t components = {n}')\n",
    "    return X_CE\n",
    "\n",
    "def Isomap(X, y=None, n=2):\n",
    "    from sklearn.manifold import Isomap\n",
    "#     print('CE Model: Isomap')\n",
    "    embedding =  Isomap(n_components=n, n_neighbors=10)\n",
    "    X_CE = embedding.fit_transform(X)\n",
    "    print(f'ISOMAP\\tOld shape = {X.shape}\\t\\t new shape = {X_CE.shape}\\t\\t components = {n}')\n",
    "    return X_CE\n",
    "\n",
    "def TSNE(X, y=None, n=2):\n",
    "    from sklearn.manifold import TSNE\n",
    "#     print('CE Model: TSNE')\n",
    "    embedding = TSNE(n_components=2, perplexity=30, n_iter=1000)\n",
    "    X_CE = embedding.fit_transform(X)\n",
    "    print(f'TSNE\\tOld shape = {X.shape}\\t\\t new shape = {X_CE.shape}\\t\\t components = {n}')\n",
    "    return X_CE\n",
    "\n",
    "\n",
    "\n",
    "def CFS(X, y, n=2):\n",
    "#     print('CFS Model')\n",
    "    selector = SelectKBest(score_func=f_regression, k=5)\n",
    "    X_new = selector.fit_transform(X, y)\n",
    "    return X_new\n",
    "\n",
    "def LLCFS(X, y=None,n=2):\n",
    "#     print('LLCFS Model')\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    return X\n",
    "\n",
    "def ILFS(X, y):\n",
    "    # create a linear regression model\n",
    "#     print('ILFS Model')\n",
    "    model = LinearRegression()\n",
    "    \n",
    "    # define the search space\n",
    "    k_features = np.arange(1, X.shape[1]+1)\n",
    "    \n",
    "    # create a sequential feature selector object\n",
    "    selector = SequentialFeatureSelector(model, k_features=k_features, forward=True, scoring='r2', cv=5)\n",
    "    \n",
    "    # perform incremental feature selection\n",
    "    selector.fit(X, y)\n",
    "    \n",
    "    # print the selected feature indices\n",
    "    print(\"Indices of selected features:\", selector.k_feature_idx_)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9250ece6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:289: RuntimeWarning: invalid value encountered in true_divide\n",
      "  correlation_coefficient /= X_norms\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:780: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.10146162e-01 1.84093080e-01 8.20609547e-02 2.12060503e-02\n",
      " 1.32595793e-03 3.80166837e-04 3.03337886e-04 1.59616295e-04\n",
      " 1.33089911e-04 7.78932435e-05]\n",
      "PCA\tNoF = 10\n",
      "CE\tOld shape = (303, 13)\t\t new shape = (303, 10)\t\t components = 10\n",
      "CE2\tOld shape = (303, 13)\t\t new shape = (303, 10)\t\t components = 10\n",
      "LLE\tOld shape = (303, 13)\t\t new shape = (303, 10)\t\t components = 10\n",
      "ISOMAP\tOld shape = (303, 13)\t\t new shape = (303, 10)\t\t components = 10\n",
      "TSNE\tOld shape = (303, 13)\t\t new shape = (303, 2)\t\t components = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFS\n",
      "LLCFS\n",
      "PCA_model\n",
      "CE_Model\n",
      "CE2\n",
      "LLE\n",
      "Isomap\n",
      "TSNE\n",
      "LinearSVM\n",
      "          DR  accuracy  sensitivity  specificity\n",
      "0        CFS  0.571429     0.255294     0.832244\n",
      "0      LLCFS  0.637363     0.371961     0.880598\n",
      "0  PCA_model  0.560440     0.331225     0.878966\n",
      "0   CE_Model  0.527473     0.200000     0.800000\n",
      "0        CE2  0.527473     0.200000     0.800000\n",
      "0        LLE  0.538462     0.211765     0.808548\n",
      "0     Isomap  0.439560     0.301029     0.832505\n",
      "0       TSNE  0.527473     0.200000     0.800000\n",
      "\n",
      "GaussianSVM\n",
      "          DR  accuracy  sensitivity  specificity\n",
      "1        CFS  0.527473     0.200000     0.800000\n",
      "1      LLCFS  0.593407     0.307794     0.869491\n",
      "1  PCA_model  0.538462     0.211765     0.804651\n",
      "1   CE_Model  0.538462     0.211765     0.818524\n",
      "1        CE2  0.538462     0.211765     0.818290\n",
      "1        LLE  0.527473     0.200000     0.804068\n",
      "1     Isomap  0.527473     0.200000     0.805845\n",
      "1       TSNE  0.527473     0.200000     0.800000\n",
      "\n",
      "PolySVM\n",
      "          DR  accuracy  sensitivity  specificity\n",
      "2        CFS  0.527473     0.200000     0.800000\n",
      "2      LLCFS  0.527473     0.289363     0.853958\n",
      "2  PCA_model  0.538462     0.220000     0.817276\n",
      "2   CE_Model  0.516484     0.203431     0.822314\n",
      "2        CE2  0.538462     0.211765     0.824836\n",
      "2        LLE  0.516484     0.223529     0.830558\n",
      "2     Isomap  0.538462     0.229167     0.829308\n",
      "2       TSNE  0.527473     0.200000     0.800000\n",
      "\n",
      "SigmoidSVM\n",
      "          DR  accuracy  sensitivity  specificity\n",
      "3        CFS  0.527473     0.200000     0.800000\n",
      "3      LLCFS  0.626374     0.338824     0.881684\n",
      "3  PCA_model  0.494505     0.210294     0.819637\n",
      "3   CE_Model  0.538462     0.216667     0.808890\n",
      "3        CE2  0.527473     0.215833     0.804130\n",
      "3        LLE  0.538462     0.224265     0.808548\n",
      "3     Isomap  0.461538     0.182598     0.804420\n",
      "3       TSNE  0.450549     0.178431     0.788875\n",
      "\n",
      "NaiveBayes\n",
      "          DR  accuracy  sensitivity  specificity\n",
      "4        CFS  0.527473     0.286127     0.873872\n",
      "4      LLCFS  0.516484     0.311961     0.872932\n",
      "4  PCA_model  0.615385     0.336225     0.879135\n",
      "4   CE_Model  0.483516     0.290833     0.836421\n",
      "4        CE2  0.483516     0.266765     0.847249\n",
      "4        LLE  0.505495     0.290294     0.837221\n",
      "4     Isomap  0.516484     0.259167     0.834310\n",
      "4       TSNE  0.516484     0.211029     0.826524\n",
      "\n",
      "Logistic\n",
      "          DR  accuracy  sensitivity  specificity\n",
      "5        CFS  0.582418     0.327696     0.871789\n",
      "5      LLCFS  0.604396     0.355490     0.884289\n",
      "5  PCA_model  0.615385     0.352059     0.885573\n",
      "5   CE_Model  0.527473     0.200000     0.800000\n",
      "5        CE2  0.527473     0.200000     0.800000\n",
      "5        LLE  0.527473     0.200000     0.800000\n",
      "5     Isomap  0.505495     0.227598     0.828025\n",
      "5       TSNE  0.527473     0.200000     0.800000\n",
      "\n",
      "CART\n",
      "          DR  accuracy  sensitivity  specificity\n",
      "6        CFS  0.505495     0.226961     0.858404\n",
      "6      LLCFS  0.505495     0.242157     0.860352\n",
      "6  PCA_model  0.582418     0.342549     0.876799\n",
      "6   CE_Model  0.472527     0.219363     0.818171\n",
      "6        CE2  0.384615     0.200294     0.812090\n",
      "6        LLE  0.439560     0.232990     0.833368\n",
      "6     Isomap  0.450549     0.222598     0.824734\n",
      "6       TSNE  0.516484     0.236029     0.830091\n",
      "\n",
      "kNN\n",
      "          DR  accuracy  sensitivity  specificity\n",
      "7        CFS  0.571429     0.295490     0.854766\n",
      "7      LLCFS  0.560440     0.274559     0.865218\n",
      "7  PCA_model  0.527473     0.231029     0.830503\n",
      "7   CE_Model  0.549451     0.243627     0.836501\n",
      "7        CE2  0.549451     0.255196     0.839697\n",
      "7        LLE  0.483516     0.203431     0.807301\n",
      "7     Isomap  0.505495     0.215098     0.817771\n",
      "7       TSNE  0.560440     0.247794     0.844519\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "# performing experiment 2    \n",
    "\n",
    "DR = [ CFS, LLCFS, PCA_model, CE_Model, CE2, LLE, Isomap, TSNE,]\n",
    "DR_TXT = [ 'CFS', 'LLCFS', 'PCA_model', 'CE_Model', 'CE2', 'LLE', 'Isomap', 'TSNE', ]\n",
    "\n",
    "# for lol in range(2, nX1.shape[1]):\n",
    "\n",
    "NoF = nX1.shape[1]-3 # Number of features\n",
    "X1s = [d(X1, np.ravel(Y1), NoF) for d in DR] # dimension reduction\n",
    "results1 = [predict(x1, np.ravel(Y1)) for x1 in X1s] # Machine learning models\n",
    "\n",
    "finals = []\n",
    "for i in range(len(DR)):\n",
    "    rd = DR_TXT[i]\n",
    "    print(rd)\n",
    "    acc, sen, spe = [],[],[]\n",
    "    for a, s, e in results1[i]:\n",
    "        acc.append(a)\n",
    "        sen.append(s)\n",
    "        spe.append(e)\n",
    "    finals.append(pd.DataFrame({'model': predictorsTXT,'accuracy': acc, 'sensitivity': sen, 'specificity': spe}))\n",
    "\n",
    "\n",
    "finalR = {}\n",
    "for i in predictorsTXT:\n",
    "    D = pd.DataFrame() # create an empty DataFrame to hold the filtered rows     \n",
    "    for df in finals:\n",
    "        row = df[df['model'] == i]\n",
    "        D = D.append(row) # filter rows that match a certain condition and append them to D\n",
    "    D['DR'] = DR_TXT\n",
    "    D = D.drop('model', axis=1)\n",
    "    finalR[i] = D\n",
    "\n",
    "for i in finalR:\n",
    "    print(i)\n",
    "    print(finalR[i][['DR', 'accuracy' , 'sensitivity',  'specificity']])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5e23cabc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:289: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  correlation_coefficient /= X_norms\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:289: RuntimeWarning: invalid value encountered in true_divide\n",
      "  correlation_coefficient /= X_norms\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:358: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f_statistic = corr_coef_squared / (1 - corr_coef_squared) * deg_of_freedom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.70211400e-01 2.74629966e-02 1.48359088e-03 4.46908849e-04\n",
      " 1.58541891e-04 8.10838008e-05 5.88920834e-05 2.96551435e-05\n",
      " 1.70482439e-05 1.65405893e-05 9.67363516e-06 7.64448221e-06\n",
      " 6.60004303e-06 3.00993914e-06 2.52564009e-06 2.05147205e-06\n",
      " 9.78104687e-07 5.89967452e-07 1.12432538e-07 8.82006134e-08\n",
      " 6.81284103e-08 4.77926180e-33 4.77926180e-33 4.77926180e-33\n",
      " 4.77926180e-33 4.77926180e-33 4.77926180e-33 4.77926180e-33\n",
      " 4.77926180e-33 4.77926180e-33 4.77926180e-33 4.77926180e-33\n",
      " 4.77926180e-33]\n",
      "PCA\tNoF = 33\n",
      "CE\tOld shape = (2126, 36)\t\t new shape = (2126, 33)\t\t components = 33\n",
      "CE2\tOld shape = (2126, 36)\t\t new shape = (2126, 33)\t\t components = 33\n",
      "LLE\tOld shape = (2126, 36)\t\t new shape = (2126, 33)\t\t components = 33\n",
      "ISOMAP\tOld shape = (2126, 36)\t\t new shape = (2126, 33)\t\t components = 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:780: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSNE\tOld shape = (2126, 36)\t\t new shape = (2126, 2)\t\t components = 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVM\n",
      "          DR  accuracy  sensitivity  specificity\n",
      "0        CFS  0.642633     0.410568     0.956995\n",
      "0      LLCFS  0.866771     0.776182     0.984684\n",
      "0  PCA_model  0.573668     0.353292     0.949094\n",
      "0   CE_Model  0.264890     0.100000     0.900000\n",
      "0        CE2  0.264890     0.100000     0.900000\n",
      "0        LLE  0.308777     0.140283     0.908689\n",
      "0     Isomap  0.148903     0.101308     0.900173\n",
      "0       TSNE  0.175549     0.076618     0.899687\n",
      "\n",
      "GaussianSVM\n",
      "          DR  accuracy  sensitivity  specificity\n",
      "1        CFS  0.692790     0.499750     0.963640\n",
      "1      LLCFS  0.876176     0.763755     0.985603\n",
      "1  PCA_model  0.291536     0.116397     0.904491\n",
      "1   CE_Model  0.305643     0.156358     0.910279\n",
      "1        CE2  0.294671     0.142320     0.908760\n",
      "1        LLE  0.365204     0.199483     0.918400\n",
      "1     Isomap  0.294671     0.121477     0.905745\n",
      "1       TSNE  0.283699     0.111704     0.903646\n",
      "\n",
      "PolySVM\n",
      "          DR  accuracy  sensitivity  specificity\n",
      "2        CFS  0.694357     0.506612     0.963546\n",
      "2      LLCFS  0.891850     0.831030     0.987660\n",
      "2  PCA_model  0.335423     0.146710     0.911594\n",
      "2   CE_Model  0.305643     0.193928     0.911272\n",
      "2        CE2  0.311912     0.155045     0.910910\n",
      "2        LLE  0.394984     0.245051     0.923029\n",
      "2     Isomap  0.297806     0.124030     0.906496\n",
      "2       TSNE  0.264890     0.100000     0.900000\n",
      "\n",
      "SigmoidSVM\n",
      "          DR  accuracy  sensitivity  specificity\n",
      "3        CFS  0.250784     0.118333     0.909582\n",
      "3      LLCFS  0.757053     0.634810     0.972332\n",
      "3  PCA_model  0.242947     0.123767     0.905476\n",
      "3   CE_Model  0.293103     0.142766     0.908995\n",
      "3        CE2  0.257053     0.121616     0.904068\n",
      "3        LLE  0.288401     0.153913     0.909905\n",
      "3     Isomap  0.268025     0.121159     0.904756\n",
      "3       TSNE  0.222571     0.099809     0.899868\n",
      "\n",
      "NaiveBayes\n",
      "          DR  accuracy  sensitivity  specificity\n",
      "4        CFS  0.655172     0.610919     0.963235\n",
      "4      LLCFS  0.705329     0.729702     0.969132\n",
      "4  PCA_model  0.752351     0.705022     0.972337\n",
      "4   CE_Model  0.225705     0.235804     0.911084\n",
      "4        CE2  0.141066     0.210823     0.906216\n",
      "4        LLE  0.211599     0.244193     0.910583\n",
      "4     Isomap  0.289969     0.272594     0.916540\n",
      "4       TSNE  0.272727     0.104237     0.901254\n",
      "\n",
      "Logistic\n",
      "          DR  accuracy  sensitivity  specificity\n",
      "5        CFS  0.746082     0.599104     0.970099\n",
      "5      LLCFS  0.874608     0.808211     0.985911\n",
      "5  PCA_model  0.517241     0.410257     0.943472\n",
      "5   CE_Model  0.264890     0.100000     0.900000\n",
      "5        CE2  0.264890     0.100000     0.900000\n",
      "5        LLE  0.266458     0.100847     0.900234\n",
      "5     Isomap  0.302508     0.245021     0.917492\n",
      "5       TSNE  0.264890     0.100000     0.900000\n",
      "\n",
      "CART\n",
      "          DR  accuracy  sensitivity  specificity\n",
      "6        CFS  0.843260     0.682329     0.981998\n",
      "6      LLCFS  0.865204     0.697285     0.984284\n",
      "6  PCA_model  0.641066     0.498634     0.959079\n",
      "6   CE_Model  0.271160     0.166076     0.905429\n",
      "6        CE2  0.296238     0.155036     0.911291\n",
      "6        LLE  0.326019     0.202469     0.913268\n",
      "6     Isomap  0.299373     0.165567     0.910955\n",
      "6       TSNE  0.286834     0.153489     0.909477\n",
      "\n",
      "kNN\n",
      "          DR  accuracy  sensitivity  specificity\n",
      "7        CFS  0.711599     0.557791     0.965729\n",
      "7      LLCFS  0.804075     0.691768     0.977348\n",
      "7  PCA_model  0.341693     0.218845     0.916834\n",
      "7   CE_Model  0.277429     0.162155     0.908439\n",
      "7        CE2  0.291536     0.164342     0.910983\n",
      "7        LLE  0.344828     0.245515     0.917584\n",
      "7     Isomap  0.322884     0.201887     0.914540\n",
      "7       TSNE  0.285266     0.157639     0.909915\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Documents\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "nX2 = stats.zscore(X2)\n",
    "\n",
    "NoF = nX2.shape[1]-3 # Number of features\n",
    "X2s = [d(X2, np.ravel(Y2), NoF) for d in DR]\n",
    "\n",
    "results2 = [predict(x2, np.ravel(Y2)) for x2 in X2s]\n",
    "finals = []\n",
    "for i in range(len(DR)):\n",
    "    rd = DR_TXT[i]\n",
    "    acc, sen, spe = [],[],[]\n",
    "    for a, s, e in results2[i]:\n",
    "        acc.append(a)\n",
    "        sen.append(s)\n",
    "        spe.append(e)\n",
    "    finals.append(pd.DataFrame({'model': predictorsTXT,'accuracy': acc, 'sensitivity': sen, 'specificity': spe}))\n",
    "    \n",
    "    \n",
    "finalR = {}\n",
    "for i in predictorsTXT:\n",
    "    D = pd.DataFrame() # create an empty DataFrame to hold the filtered rows     \n",
    "    for df in finals:\n",
    "        row = df[df['model'] == i]\n",
    "        D = D.append(row) # filter rows that match a certain condition and append them to D\n",
    "    D['DR'] = DR_TXT\n",
    "    D = D.drop('model', axis=1)\n",
    "    finalR[i] = D\n",
    "\n",
    "for i in finalR:\n",
    "    print(i)\n",
    "    print(finalR[i][['DR', 'accuracy' , 'sensitivity',  'specificity']])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9a8448",
   "metadata": {},
   "outputs": [],
   "source": [
    "DR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
